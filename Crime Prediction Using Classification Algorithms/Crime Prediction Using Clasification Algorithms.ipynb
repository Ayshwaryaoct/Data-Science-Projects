{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ayshwarya Sambasivan\n",
    "##### Crime prediction Using Classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, Ridge \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "# since we are performing cross validation and classification reports on a number of models, a wrapper class \n",
    "# is used to run the cross_val_score function and extract scores required\n",
    "class CS595ModelClassificationScoring(object):\n",
    "    def __init__(self, model_object=None, train_data=None, target_data=None, model_name=None):\n",
    "        self.model_object = model_object\n",
    "        self.train_data = train_data\n",
    "        self.target_data = target_data\n",
    "        self.model_name = model_name if model_name is not None else \"model\"\n",
    "        self.full_model = None\n",
    "    def cross_validate(self, num_folds=10, scorers=[\"accuracy\", \"precision\", \"recall\", \"f1\"]):\n",
    "        print(\"\\n{} performace on cross validation:\".format(self.model_name))\n",
    "        all_scores = []\n",
    "        for scorer in scorers:\n",
    "            scores = cross_val_score(copy.deepcopy(self.model_object),  # steps to convert raw messages into models\n",
    "                                     self.train_data,  # training data\n",
    "                                     self.target_data,  # training labels\n",
    "                                     cv=num_folds,# split data randomly into num_folds parts: num_folds-1 for training, 1 for scoring\n",
    "                                     scoring=scorer,  # scoring metric\n",
    "                                     n_jobs=-1,  # -1 = use all cores = faster\n",
    "                                     )\n",
    "            all_scores.append({'score_name': scorer, 'scores': scores, 'mean': scores.mean(), 'std': scores.std()})\n",
    "        self.print_cross_val_scores(num_folds, self.model_name, all_scores)\n",
    "\n",
    "    def full_data_fit_score(self, target_names=None, print_report='clf'):\n",
    "        self.full_model = copy.deepcopy(self.model_object)\n",
    "        self.full_model.fit(self.train_data, self.target_data)\n",
    "        if print_report=='clf':\n",
    "            print(\"\\n{} performace on training with full data:\".format(self.model_name))\n",
    "            print_classification_report(self.full_model, self.train_data, self.target_data, target_names)\n",
    "        else: \n",
    "            print(\"\\n{} performace on training with full data:\".format(self.model_name))\n",
    "            predicted_data = self.full_model.predict(self.train_data)\n",
    "            print(\"\\tMSE of the model is {}\".format(mean_squared_error(self.target_data, predicted_data)))\n",
    "            print(u\"\\tR\\u00B2 of the model is {}\".format(r2_score(self.target_data, predicted_data)))\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.score_dict:\n",
    "            for score_name in self.score_names:\n",
    "                yield self.score_dict[score_name]\n",
    "\n",
    "    def print_cross_val_scores(self, num_folds, model_name, model_scores):\n",
    "        print(\"\\nThe {}-fold cross-validation {} for the {} model are: \".format(num_folds, ', '.join(\n",
    "            [scores['score_name'] for scores in model_scores]), model_name))\n",
    "        for scores in model_scores:\n",
    "            print(u\"\\t{:{}s}: {:{}f}(\\u00B1{})\".format(scores['score_name'], 22, round(scores['mean'], 4), 2,\n",
    "                                                       round(scores['std'], 2)))\n",
    "\n",
    "\n",
    "# helper function to calculate and print classification report            \n",
    "def print_classification_report(trained_model_object, train_data, target_data, target_names):\n",
    "    predicted_data = trained_model_object.predict(train_data)\n",
    "    print(\"Accuracy of the model is {}\".format(accuracy_score(target_data, predicted_data)))\n",
    "    print(classification_report(target_data, predicted_data, target_names=target_names))\n",
    "\n",
    "\n",
    "# helper function to print feature importances\n",
    "def show_feature_importances(importances_list, variable_list, model_name=\"\", topn=10, sort_order_ascending=False, print_leading=True):\n",
    "    feature_importances = pd.DataFrame(importances_list, columns=[\"Feature Importance\"], index=variable_list) \\\n",
    "        .sort_values([\"Feature Importance\"], ascending=sort_order_ascending)\n",
    "    if print_leading:\n",
    "        print(\"\\nThe 10 most predictive features of the {} are : \".format(model_name))\n",
    "    print(feature_importances.dropna().head(topn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>communityname</th>\n",
       "      <th>fold</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>...</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabastercity</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AlexanderCitycity</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Annistoncity</td>\n",
       "      <td>3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Athenscity</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Auburncity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   state      communityname  fold  population  householdsize  racepctblack  \\\n",
       "0      1      Alabastercity     7        0.01           0.61          0.21   \n",
       "1      1  AlexanderCitycity    10        0.01           0.41          0.55   \n",
       "2      1       Annistoncity     3        0.03           0.34          0.86   \n",
       "3      1         Athenscity     8        0.01           0.38          0.35   \n",
       "4      1         Auburncity     1        0.04           0.37          0.32   \n",
       "\n",
       "   racePctWhite  racePctAsian  racePctHisp  agePct12t21         ...           \\\n",
       "0          0.83          0.02         0.01         0.41         ...            \n",
       "1          0.57          0.01         0.00         0.47         ...            \n",
       "2          0.30          0.04         0.01         0.41         ...            \n",
       "3          0.71          0.04         0.01         0.39         ...            \n",
       "4          0.70          0.21         0.02         1.00         ...            \n",
       "\n",
       "   PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
       "0            0.03              0.70            0.40           0.34   \n",
       "1            0.00              0.93            0.66           0.82   \n",
       "2            0.04              0.77            0.59           0.70   \n",
       "3            0.03              0.78            0.56           0.67   \n",
       "4            0.12              0.49            0.12           0.00   \n",
       "\n",
       "   PctSameState85  LandArea  PopDens  PctUsePubTrans  LemasPctOfficDrugUn  \\\n",
       "0            0.57      0.05     0.06            0.01                  0.0   \n",
       "1            0.84      0.11     0.03            0.01                  0.0   \n",
       "2            0.64      0.06     0.11            0.04                  0.0   \n",
       "3            0.71      0.09     0.05            0.00                  0.0   \n",
       "4            0.15      0.09     0.09            0.01                  0.0   \n",
       "\n",
       "   ViolentCrimesPerPop  \n",
       "0                 0.06  \n",
       "1                 0.14  \n",
       "2                 1.00  \n",
       "3                 0.23  \n",
       "4                 0.15  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize some dictionaries to hold scorers and models\n",
    "scorers = {}\n",
    "\n",
    "# set up data file paths\n",
    "data_folder = 'Crime Prediction Data'\n",
    "data_path = os.path.join(os.path.dirname(os.path.realpath('__file__')), data_folder)\n",
    "full_data_file = os.path.join(data_path, 'communities-crime-full.csv')\n",
    "clean_data_file = os.path.join(data_path, 'communities-crime-clean.csv')\n",
    "\n",
    "# read clean data file\n",
    "clean_data_df = pd.read_csv(clean_data_file)\n",
    "\n",
    "# set up predictor variables and target variables\n",
    "target_variable_clf = 'highCrime'\n",
    "target_variable_reg = 'ViolentCrimesPerPop'\n",
    "non_predictors = {target_variable_reg, target_variable_clf, 'communityname', 'fold', 'state', 'county', 'community'} \n",
    "predictor_variables =  [col for col in clean_data_df if col not in non_predictors]\n",
    "clf_target_names = ['Low crime', 'High crime']\n",
    "\n",
    "clean_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\tDecision Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive instances of high crime (per capita crime rate > 0.1) is 62.72% \n",
      "Percentage of negative instances of high crime (per capita crime rate <= 0.1) is 37.28% \n"
     ]
    }
   ],
   "source": [
    "# percentage of positive and negative instances of per capita crime rate (ViolentCrimesPerPop) greater than 0.1\n",
    "clean_data_df[target_variable_clf] = clean_data_df['ViolentCrimesPerPop'] > 0.1\n",
    "pct_high_crime = clean_data_df[target_variable_clf].value_counts(normalize=True) * 100\n",
    "print(\"Percentage of positive instances of high crime (per capita crime rate > 0.1) is {0:.2f}% \".format(pct_high_crime[True]))\n",
    "print(\"Percentage of negative instances of high crime (per capita crime rate <= 0.1) is {0:.2f}% \".format(pct_high_crime[False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree performace on training with full data:\n",
      "Accuracy of the model is 1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  Low crime       1.00      1.00      1.00       743\n",
      " High crime       1.00      1.00      1.00      1250\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decision tree to predict highCrime on the entire dataset \n",
    "# initializing the classification scoring instance for decision tree\n",
    "scorers['Decision Tree'] = CS595ModelClassificationScoring(\n",
    "                                DecisionTreeClassifier(random_state=210), \n",
    "                                clean_data_df[predictor_variables], \n",
    "                                clean_data_df[target_variable_clf], \n",
    "                                'Decision Tree')\n",
    "\n",
    "# training accuracy, precision, and recall on full data\n",
    "scorers['Decision Tree'].full_data_fit_score(clf_target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 10 most predictive features of the Decision Tree are : \n",
      "                 Feature Importance\n",
      "PctKids2Par                0.362098\n",
      "racePctWhite               0.089608\n",
      "racePctHisp                0.048585\n",
      "PctLess9thGrade            0.020673\n",
      "PctEmplManu                0.017035\n",
      "PctSameState85             0.016741\n",
      "MedRent                    0.013290\n",
      "PctPersOwnOccup            0.012138\n",
      "HousVacant                 0.011821\n",
      "PctImmigRec10              0.011378\n"
     ]
    }
   ],
   "source": [
    "# feature importances\n",
    "show_feature_importances(scorers['Decision Tree'].full_model.feature_importances_, \n",
    "                         predictor_variables, 'Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The percentage of kids in family housing with two parents is the most important feature by a long way. At first glance it does not seem to make sense. But its been concluded by social scientists that children raised in intact married families are more likely to attend college, are less likely to be physically or sexually abused, less likely to use drugs or alcohol and to commit delinquent behaviors, and are less likely to be raised in poverty (\"Why Marriage Matters: 26 Conclusions from the Social Sciences,\" Bradford Wilcox, Institute for American Values). These positive effects I think has a cascading effect on the level of crime in the locality. So I think that the importance of this feature is justified as it seems to have along term effect on the people in the locality. \n",
    "\n",
    "2. Other features such as PctLess9thGrade (indicative of lower level of education), PctEmplManu (indicative of lower/erratic income and education level) having high importance makes sense in that people without a steady income and poor education level may be more prone to commit crimes to obtain money. \n",
    "\n",
    "3. Features like PctPersOwnOccup, racePctWhite and racePctHisp (a person's economic class position is correlated with one's racial background) are important because the class level of majority of a locality would have a big impact on the crime level. e.g. people of white race overrepresented among middle and higher-income people (in US). So a locality with high racePctWhite may have a higher average income which maybe pushing crime rates low.\n",
    "\n",
    "4. Features such as MedRent and HousVacant are affected by the crime rate in a locality. e.g. Housing in low crime locality maybe be more in demand pushing rents up and high crime rates may discourage people from renting houses in the area leaving more homes vacant.\n",
    "\n",
    "5. PctImmigRec10 and PctSameState85 indicate the level of long term residents of a locality. This makes sense because a larger number of long term residents means that they probably have a steady income and proper shelter which may reduce crime rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the Decision Tree model are: \n",
      "\taccuracy              : 0.735000(±0.05)\n",
      "\tprecision             : 0.793900(±0.03)\n",
      "\trecall                : 0.781600(±0.09)\n",
      "\tf1                    : 0.784900(±0.05)\n"
     ]
    }
   ],
   "source": [
    "# cross validation scores : mean of measures \"accuracy\", \"precision\", \"recall\" and \"f1\" of 10-fold cross validation\n",
    "scorers['Decision Tree'].cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree in the trained with all the data perfectly memorises all the data. i.e. All the nodes are 100% pure. With a more stringent set of parameters like lower max_depth or lower max_leaf_nodes the decision tree would not be able to do that.\n",
    "Further the test cases have already been used to train the tree, so there is no new set of observations that the tree has to predict. In other words the decision tree has been trained to perfectly fit (overfit) all the data available. Hence all the scores are 1.\n",
    "\n",
    "But in the case of cross validation a part of data not used for training. The trained decision tree is therefore unlikely to fit the unseen data perfectly and so the accuracy, precision and recall are lower than the model trained on all the data. These scores are better indicators to the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\tLinear Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gaussian Naive Bayes performace on training with full data:\n",
      "Accuracy of the model is 0.7782237832413447\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  Low crime       0.64      0.91      0.75       743\n",
      " High crime       0.93      0.70      0.80      1250\n",
      "\n",
      "avg / total       0.82      0.78      0.78      1993\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the Gaussian Naive Bayes model are: \n",
      "\taccuracy              : 0.761600(±0.05)\n",
      "\tprecision             : 0.911800(±0.06)\n",
      "\trecall                : 0.692000(±0.09)\n",
      "\tf1                    : 0.781600(±0.06)\n"
     ]
    }
   ],
   "source": [
    "# initializing the classification scorer instance for Gaussian Naive Bayes classifier\n",
    "scorers['Gaussian Naive Bayes'] = CS595ModelClassificationScoring(GaussianNB(), \n",
    "                                clean_data_df[predictor_variables], \n",
    "                                clean_data_df[target_variable_clf], \n",
    "                                'Gaussian Naive Bayes')\n",
    "\n",
    "# training accuracy, precision, and recall on full data\n",
    "# calling full_data_fit_score on scorer returns model trained on all the data and prints its classification performance\n",
    "scorers['Gaussian Naive Bayes'].full_data_fit_score(clf_target_names)\n",
    "\n",
    "# 10-fold cross-validation accuracy, precision, and recall \n",
    "scorers['Gaussian Naive Bayes'].cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 10 most predictive features of the Gaussian Naive Bayes are : \n",
      "                  Feature Importance\n",
      "PctKids2Par                 0.809748\n",
      "PctFam2Par                  0.745545\n",
      "racePctWhite                0.735230\n",
      "PctIlleg                    0.709261\n",
      "FemalePctDiv                0.693978\n",
      "TotalPctDiv                 0.674645\n",
      "PctYoungKids2Par            0.665009\n",
      "pctWInvInc                  0.661076\n",
      "PctTeen2Par                 0.642949\n",
      "MalePctDivorce              0.616864\n"
     ]
    }
   ],
   "source": [
    "# extracting feature importance from model attributes theata_(mean of each feature per class)\n",
    "# and sigma_ (variance of each feature per class)\n",
    "class_means_of_variables = pd.DataFrame(scorers['Gaussian Naive Bayes'].full_model.theta_.T)\n",
    "class_means_of_variables['abs_mean_diffrence'] = (class_means_of_variables[0]-class_means_of_variables[1]).abs()\n",
    "class_std_of_variables = pd.DataFrame(scorers['Gaussian Naive Bayes'].full_model.sigma_.T)\n",
    "class_std_of_variables['sum_of_std_dev'] =  (class_std_of_variables[0].apply(np.sqrt)+class_std_of_variables[1].apply(np.sqrt))\n",
    "feature_importances_nb = (class_means_of_variables['abs_mean_diffrence']/class_std_of_variables['sum_of_std_dev']).tolist()\n",
    "\n",
    "show_feature_importances(feature_importances_nb, predictor_variables, 'Gaussian Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances in the Gaussian Naive Bayes classifier seem all related to family and kids. \n",
    "1. PctKids2Par, PctFam2Par, PctIlleg, PctYoungKids2Par and PctTeen2Par are all indicative of the ratio of dependants to independants in the family. Two parent families may have more stable form of income while single parent familes are more likely to be at an economic disadvantage. e.g. Some parents may have less income to spend on children and their upbrining. So these variables all make sense in terms of being predictive of the the crime rate since they affect the children in the long term and adults in the short term. \n",
    "\n",
    "2. FemalePctDiv, TotalPctDiv and MalePctDivorce are again all indicative of state of families in the locality. These values maybe aliasing economic and social factors. e.g. low income families are more likely to divorce and free individuals from such divorces maybe more susceptble to fall into criminal behaviour. So these variables actually make sense.\n",
    "\n",
    "3. racePctWhite, pctWInvInc are indicative of the economic class of the people in the locality especially when the values are extremely low or high. A better economic class would lead to a lower crime rate. So these variables make sense being predictive of the crime rate.\n",
    "\n",
    "As compared to the feature importances of the decision tree only a few variables overlap: PctKids2Par, racePctWhite. This maybe because of the difference in which decision trees and Gaussian Naive Bayes treats correlated features. Decision tree would only pick the feature that provides most information from a correlated group. Since the information gain from successive correalated featues would be keep decreasing those successive features would not be picked. Whereas Gaussian NB treats all features are independant ones. Hence we see seemingly correlated features like FemalePctDiv, TotalPctDiv and MalePctDivorce all having high weights. But the variables themselves are representing the same facets such as average economic class of residents and ratio of dependants to independants in the family.\n",
    "\n",
    "The average accuracy and precision from cross validation of the Gaussian Naive Bayes classifier are actually higher than that of the Decision Tree while its recall is poorer. This means they have an almost identical F1 score. So we can say the classifiers are different but their performances are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear SVC performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the Linear SVC model are: \n",
      "\taccuracy              : 0.796200(±0.05)\n",
      "\tprecision             : 0.845400(±0.05)\n",
      "\trecall                : 0.834400(±0.12)\n",
      "\tf1                    : 0.833400(±0.06)\n",
      "\n",
      "Linear SVC performace on training with full data:\n",
      "Accuracy of the model is 0.8504766683391871\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  Low crime       0.80      0.81      0.80       743\n",
      " High crime       0.88      0.88      0.88      1250\n",
      "\n",
      "avg / total       0.85      0.85      0.85      1993\n",
      "\n",
      "\n",
      "The 10 most predictive features of the Linear SVC are : \n",
      "                  Feature Importance\n",
      "pctWInvInc                  1.888486\n",
      "PersPerOccupHous            1.755127\n",
      "racePctWhite                1.500217\n",
      "PctKids2Par                 1.190329\n",
      "RentHighQ                   1.066885\n",
      "MalePctDivorce              1.065696\n",
      "NumUnderPov                 1.051548\n",
      "NumStreet                   1.019155\n",
      "PctOccupMgmtProf            1.014672\n",
      "population                  1.002301\n"
     ]
    }
   ],
   "source": [
    "# initializing the classification scorer instance for Linear SVC classifier\n",
    "scorers['Linear SVC'] = CS595ModelClassificationScoring(LinearSVC(), \n",
    "                                clean_data_df[predictor_variables], \n",
    "                                clean_data_df[target_variable_clf], \n",
    "                                'Linear SVC')\n",
    "\n",
    "# 10-fold cross-validation accuracy, precision, and recall \n",
    "scorers['Linear SVC'].cross_validate()\n",
    "\n",
    "# training accuracy, precision, and recall on full data\n",
    "# calling full_data_fit_score on scorer returns model trained on all the data and prints its classification performance\n",
    "scorers['Linear SVC'].full_data_fit_score(clf_target_names)\n",
    "\n",
    "# feature importances \n",
    "show_feature_importances(np.abs(scorers['Linear SVC'].full_model.coef_.T), predictor_variables, 'Linear SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features make sense and a well balanced mix of factors. \n",
    "1. PersPerOccupHous, NumUnderPov and NumStreet are indicators of the economic class of the people in the locality especially those that are not doing too well. When these variables have high values the economic condition of a majority of the persons in these localities are bad. This maybe  leading to a high level of crime.\n",
    "\n",
    "2. PctKids2Par is indicative of kids growing up in a stable environment which as described in the Decision Trees section has a lot of positive effects on how they turn out as adults. \n",
    "\n",
    "3. pctWInvInc, racePctWhite, RentHighQ and PctOccupMgmtProf are indicators of good economic class. When these are high one could expect a locality with a high percapita income and therefore a low crime rate.\n",
    "\n",
    "4. MalePctDivorce maybe indicative of men who are not restrained down by familial responsibilities who are in a maybe not in the best emotional state and are more susceptible to commit a crime.\n",
    "\n",
    "5. population in general is indicative of living conditions and economic class.  Its possible that when population is high the level of crime is also high.\n",
    "\n",
    "The features of high importance in the linear SVC seem to measure different facets of the population and therefore the classification results are better than the Decision tree model. They do share a few features like PctKids2Par and racePctWhite. It has better average scores in accuracy, precision, recall anf f1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation neg_mean_squared_error, r2 for the Linear Regression model are: \n",
      "\tneg_mean_squared_error: -0.020100(±0.01)\n",
      "\tr2                    : 0.585900(±0.09)\n",
      "\n",
      "Linear Regression performace on training with full data:\n",
      "\tMSE of the model is 0.016516774880307176\n",
      "\tR² of the model is 0.6957224231409389\n",
      "\n",
      "The features are most predictive of a high crime rate are:\n",
      "                  Feature Importance\n",
      "PersPerOccupHous            0.635088\n",
      "PctHousOwnOcc               0.568133\n",
      "MalePctDivorce              0.458517\n",
      "PctRecImmig8                0.432511\n",
      "MedRent                     0.372728\n",
      "medFamInc                   0.287979\n",
      "PctEmploy                   0.248474\n",
      "MalePctNevMarr              0.226728\n",
      "PctPersDenseHous            0.214353\n",
      "OwnOccMedVal                0.212876\n",
      "\n",
      "The features are most predictive of a low crime rate are:\n",
      "                    Feature Importance\n",
      "PctPersOwnOccup              -0.675694\n",
      "TotalPctDiv                  -0.561924\n",
      "whitePerCap                  -0.351016\n",
      "PctKids2Par                  -0.322651\n",
      "OwnOccLowQuart               -0.308170\n",
      "numbUrban                    -0.296443\n",
      "PersPerRentOccHous           -0.254572\n",
      "RentLowQ                     -0.234752\n",
      "agePct12t29                  -0.229218\n",
      "PctRecImmig5                 -0.218221\n"
     ]
    }
   ],
   "source": [
    "# initializing the regression scorer instance for Linear SVC classifier\n",
    "scorers['Linear Regression'] = CS595ModelClassificationScoring(LinearRegression(), \n",
    "                                             clean_data_df[predictor_variables], \n",
    "                                             clean_data_df[target_variable_reg],\n",
    "                                            'Linear Regression')\n",
    "\n",
    "# 10-fold cross-validation accuracy, precision, and recall \n",
    "scorers['Linear Regression'].cross_validate(scorers =['neg_mean_squared_error', 'r2'])\n",
    "\n",
    "# training accuracy, precision, and recall on full data\n",
    "# calling full_data_fit_score on scorer returns model trained on all the data and prints its classification performance\n",
    "scorers['Linear Regression'].full_data_fit_score(print_report='reg')\n",
    "print(\"\\nThe features are most predictive of a high crime rate are:\")\n",
    "show_feature_importances(scorers['Linear Regression'].full_model.coef_.T, predictor_variables, 'Linear Regression model', print_leading=False)\n",
    "print(\"\\nThe features are most predictive of a low crime rate are:\")\n",
    "show_feature_importances(scorers['Linear Regression'].full_model.coef_.T, predictor_variables, 'Linear Regression model', \n",
    "                         sort_order_ascending=True,print_leading=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated mean-squared-error (MSE) of the linear regression model using 10-fold cross-validation is 0.020100(±0.01).\n",
    "\n",
    "The MSE of the linear regression model trained and tested all the data is 0.016516.\n",
    "\n",
    "I have used the magnitude of the coefficients of the linear regression model to determine which features are most predictive of a high or low crime rate. The top 10 features sorted by magnitude are shown above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 1.0\n",
      "\n",
      "Ridge Regression performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation neg_mean_squared_error, r2 for the Ridge Regression model are: \n",
      "\tneg_mean_squared_error: -0.019800(±0.01)\n",
      "\tr2                    : 0.593600(±0.09)\n",
      "\n",
      "Ridge Regression performace on training with full data:\n",
      "\tMSE of the model is 0.016763529155169477\n",
      "\tR² of the model is 0.691176632974347\n"
     ]
    }
   ],
   "source": [
    "# ridge cross validation\n",
    "ridge_cv = RidgeCV(alphas=[10, 1, 0.1, 0.01, 0.001])\n",
    "ridge_cv.fit(clean_data_df[predictor_variables], clean_data_df[target_variable_reg])       \n",
    "best_alpha = ridge_cv.alpha_\n",
    "print(\"The best alpha is {}\".format(best_alpha))\n",
    "\n",
    "scorers['Ridge Regression'] = CS595ModelClassificationScoring(Ridge(alpha = best_alpha), \n",
    "                                             clean_data_df[predictor_variables], \n",
    "                                             clean_data_df[target_variable_reg],\n",
    "                                            'Ridge Regression')\n",
    "\n",
    "# 10-fold cross-validation accuracy, precision, and recall \n",
    "scorers['Ridge Regression'].cross_validate(scorers =['neg_mean_squared_error', 'r2'])\n",
    "\n",
    "# training accuracy, precision, and recall on full data\n",
    "# calling full_data_fit_score on scorer returns model trained on all the data and prints its classification performance\n",
    "scorers['Ridge Regression'].full_data_fit_score(print_report='reg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much difference in the performance of the linear and ridge regression. From this we can conclude that there is almost no overfitting in the linear regression. If there had been overfitting it would have been penalized in the ridge regression and it would have a better MSE and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha for polynomial features is 1.0\n",
      "\n",
      "Polynomial Ridge Regression performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation neg_mean_squared_error, r2 for the Polynomial Ridge Regression model are: \n",
      "\tneg_mean_squared_error: -0.019800(±0.01)\n",
      "\tr2                    : 0.598600(±0.09)\n",
      "\n",
      "Polynomial Ridge Regression performace on training with full data:\n",
      "\tMSE of the model is 0.01225176206128106\n",
      "\tR² of the model is 0.7742939224348716\n"
     ]
    }
   ],
   "source": [
    "# initializing pipeline for creating polynomial features and fitting Ridge regression\n",
    "ridge_cv_poly = RidgeCV(alphas=[10, 1, 0.1, 0.01, 0.001])\n",
    "poly = PolynomialFeatures(2)\n",
    "ridge_cv_poly.fit(poly.fit_transform(clean_data_df[predictor_variables]), clean_data_df[target_variable_reg])       \n",
    "best_alpha_poly = ridge_cv_poly.alpha_\n",
    "print(\"The best alpha for polynomial features is {}\".format(best_alpha))\n",
    "\n",
    "poly_ridge_model = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
    "                             ('ridge', Ridge(alpha = best_alpha_poly))\n",
    "                            ])\n",
    "scorers['Polynomial Ridge Regression'] = CS595ModelClassificationScoring(poly_ridge_model, \n",
    "                                             clean_data_df[predictor_variables], \n",
    "                                             clean_data_df[target_variable_reg],\n",
    "                                            'Polynomial Ridge Regression')\n",
    "\n",
    "# 10-fold cross-validation accuracy, precision, and recall \n",
    "scorers['Polynomial Ridge Regression'].cross_validate(scorers =['neg_mean_squared_error', 'r2'])\n",
    "\n",
    "# training accuracy, precision, and recall on full data\n",
    "# calling full_data_fit_score on scorer returns model trained on all the data and prints its classification performance\n",
    "scorers['Polynomial Ridge Regression'].full_data_fit_score(print_report='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At alpha = 1, the model using quadratic features is not better than the one using linear features since they have the same average MSE and R-squared on cross validation.\n",
    "\n",
    "The MSE and R-squared are only marginally better on the model trained on full data set. So we cannot say that quadratic model is better than the linear model for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\tDirty Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>community</th>\n",
       "      <th>communityname</th>\n",
       "      <th>fold</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>...</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "      <th>highCrime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lakewoodcity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tukwilacity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aberdeentown</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>5.0</td>\n",
       "      <td>81440.0</td>\n",
       "      <td>Willingborotownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>95.0</td>\n",
       "      <td>6096.0</td>\n",
       "      <td>Bethlehemtownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   state  county  community        communityname  fold  population  \\\n",
       "0      8     NaN        NaN         Lakewoodcity     1        0.19   \n",
       "1     53     NaN        NaN          Tukwilacity     1        0.00   \n",
       "2     24     NaN        NaN         Aberdeentown     1        0.00   \n",
       "3     34     5.0    81440.0  Willingborotownship     1        0.04   \n",
       "4     42    95.0     6096.0    Bethlehemtownship     1        0.01   \n",
       "\n",
       "   householdsize  racepctblack  racePctWhite  racePctAsian    ...      \\\n",
       "0           0.33          0.02          0.90          0.12    ...       \n",
       "1           0.16          0.12          0.74          0.45    ...       \n",
       "2           0.42          0.49          0.56          0.17    ...       \n",
       "3           0.77          1.00          0.08          0.12    ...       \n",
       "4           0.55          0.02          0.95          0.09    ...       \n",
       "\n",
       "   PopDens  PctUsePubTrans  PolicCars  PolicOperBudg  LemasPctPolicOnPatr  \\\n",
       "0     0.26            0.20       0.06           0.04                  0.9   \n",
       "1     0.12            0.45        NaN            NaN                  NaN   \n",
       "2     0.21            0.02        NaN            NaN                  NaN   \n",
       "3     0.39            0.28        NaN            NaN                  NaN   \n",
       "4     0.09            0.02        NaN            NaN                  NaN   \n",
       "\n",
       "   LemasGangUnitDeploy  LemasPctOfficDrugUn  PolicBudgPerPop  \\\n",
       "0                  0.5                 0.32             0.14   \n",
       "1                  NaN                 0.00              NaN   \n",
       "2                  NaN                 0.00              NaN   \n",
       "3                  NaN                 0.00              NaN   \n",
       "4                  NaN                 0.00              NaN   \n",
       "\n",
       "   ViolentCrimesPerPop  highCrime  \n",
       "0                 0.20          1  \n",
       "1                 0.67          1  \n",
       "2                 0.43          1  \n",
       "3                 0.12          1  \n",
       "4                 0.03          0  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading full data file with '?' to denote nan\n",
    "full_data_df = pd.read_csv(full_data_file, na_values=['?'])\n",
    "\n",
    "non_predictors_dirty = {target_variable_reg, target_variable_clf, 'communityname', 'fold', 'state', 'county', 'community'} \n",
    "predictor_variables_dirty =  [col for col in full_data_df if col not in non_predictors_dirty]\n",
    "full_data_df[target_variable_clf] = (full_data_df[target_variable_reg] > 0.1).astype(int)\n",
    "\n",
    "full_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OtherPerCap           has 1 missing value\n",
      "LemasSwornFT          has 1675 missing values\n",
      "LemasSwFTPerPop       has 1675 missing values\n",
      "LemasSwFTFieldOps     has 1675 missing values\n",
      "LemasSwFTFieldPerPop  has 1675 missing values\n",
      "LemasTotalReq         has 1675 missing values\n",
      "LemasTotReqPerPop     has 1675 missing values\n",
      "PolicReqPerOffic      has 1675 missing values\n",
      "PolicPerPop           has 1675 missing values\n",
      "RacialMatchCommPol    has 1675 missing values\n",
      "PctPolicWhite         has 1675 missing values\n",
      "PctPolicBlack         has 1675 missing values\n",
      "PctPolicHisp          has 1675 missing values\n",
      "PctPolicAsian         has 1675 missing values\n",
      "PctPolicMinor         has 1675 missing values\n",
      "OfficAssgnDrugUnits   has 1675 missing values\n",
      "NumKindsDrugsSeiz     has 1675 missing values\n",
      "PolicAveOTWorked      has 1675 missing values\n",
      "PolicCars             has 1675 missing values\n",
      "PolicOperBudg         has 1675 missing values\n",
      "LemasPctPolicOnPatr   has 1675 missing values\n",
      "LemasGangUnitDeploy   has 1675 missing values\n",
      "PolicBudgPerPop       has 1675 missing values\n",
      "\n",
      "\n",
      "Total number of rows with missing data in any column is 1675\n"
     ]
    }
   ],
   "source": [
    "# adding dummy columns for missing data and count of missing data\n",
    "predictor_variables_dirty_missing = []\n",
    "\n",
    "for col in predictor_variables_dirty:\n",
    "    missing_count = full_data_df[col].isnull().sum()\n",
    "    if missing_count>0:\n",
    "        print(\"{:{}s} has {} missing {}\".format(col,21, missing_count, 'value' if missing_count==1 else 'values'))\n",
    "        full_data_df[col+'_missing'] = full_data_df[col].isnull().astype(int)\n",
    "        predictor_variables_dirty_missing.append(col+'_missing')\n",
    "\n",
    "        \n",
    "# checking if the same rows are missing data\n",
    "missing_row_indices = set()\n",
    "for col in predictor_variables_dirty_missing:\n",
    "    missing_row_indices = missing_row_indices | set(full_data_df[full_data_df[col]==True][col].index)\n",
    "print(\"\\n\\nTotal number of rows with missing data in any column is {}\".format(len(missing_row_indices)))\n",
    "\n",
    "# full_data_df.loc[missing_row_indices,target_variable_reg].max()\n",
    "# full_data_df.loc[set(full_data_df.index)-missing_row_indices, target_variable_reg].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same rows seem to be missing the same set of variables. This means that the values are not missing at random. And since the number of missing data in these columns is very high as compared to the total number of observations, its unlikely that these columns would have much predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replacing nan values in predictor with -1 which is a value outside the exisiting  range of values\n",
    "for col in predictor_variables_dirty:\n",
    "    full_data_df[col].fillna(full_data_df[col].median(), inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor_variables_dirty_new =  [col for col in full_data_df if col not in non_predictors_dirty \n",
    "#                                   and col not in predictor_variables_dirty_missing\n",
    "                                 ]\n",
    "target_variable_dirty = target_variable_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree dirty data performace on training with full data:\n",
      "Accuracy of the model is 1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  Low crime       1.00      1.00      1.00       743\n",
      " High crime       1.00      1.00      1.00      1251\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1994\n",
      "\n",
      "\n",
      "Decision Tree dirty data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the Decision Tree dirty data model are: \n",
      "\taccuracy              : 0.749700(±0.03)\n",
      "\tprecision             : 0.808800(±0.03)\n",
      "\trecall                : 0.788100(±0.03)\n",
      "\tf1                    : 0.798000(±0.02)\n"
     ]
    }
   ],
   "source": [
    "# initializing the classification scoring instance for decision tree\n",
    "scorers['Decision Tree with dirty data'] = CS595ModelClassificationScoring(DecisionTreeClassifier(random_state=210), \n",
    "                                full_data_df[predictor_variables_dirty_new], \n",
    "                                full_data_df[target_variable_clf], \n",
    "                    'Decision Tree dirty data')\n",
    "\n",
    "# training accuracy, precision, and recall on full data\n",
    "# calling full_data_fit_score on scorer returns model trained on all the data and prints its classification performance\n",
    "scorers['Decision Tree with dirty data'].full_data_fit_score(clf_target_names, print_report='clf')\n",
    "\n",
    "# 10-fold cross-validation accuracy, precision, and recall \n",
    "scorers['Decision Tree with dirty data'].cross_validate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision tree model's cross validation scores are slightly better with dirty data. The missing values actually are improving the decision tree model and is informative. It is counter intuitive that less data is actually able to predict more accurately than more data could. Its either because the variables with missing values have very little predictive power and maybe even be adding noise to the model. As stated  earlier the missing values are not missing at random and therefore are able to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with non linear kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC(rbf kernel) with clean data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the SVC(rbf kernel) with clean data model are: \n",
      "\taccuracy              : 0.818300(±0.05)\n",
      "\tprecision             : 0.856700(±0.06)\n",
      "\trecall                : 0.862400(±0.1)\n",
      "\tf1                    : 0.854200(±0.05)\n",
      "\n",
      "SVC(rbf kernel) with dirty data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the SVC(rbf kernel) with dirty data model are: \n",
      "\taccuracy              : 0.828500(±0.01)\n",
      "\tprecision             : 0.860500(±0.02)\n",
      "\trecall                : 0.868100(±0.02)\n",
      "\tf1                    : 0.863900(±0.01)\n"
     ]
    }
   ],
   "source": [
    "# setting variables for easy access\n",
    "X_clean, y_clean = clean_data_df[predictor_variables], clean_data_df[target_variable_clf]\n",
    "X_dirty, y_dirty = full_data_df[predictor_variables], full_data_df[target_variable_clf]\n",
    "\n",
    "# initializing the classification scoring instance for SVM with rbf kernel\n",
    "svc = SVC(kernel=\"rbf\", random_state =210, cache_size=400)\n",
    "\n",
    "# 10-fold cross-validation accuracy, precision, recall and f1 average scores \n",
    "scorers['SVC(rbf kernel) with clean data']=CS595ModelClassificationScoring(copy.deepcopy(svc), X_clean, y_clean, 'SVC(rbf kernel) with clean data')\n",
    "scorers['SVC(rbf kernel) with clean data'].cross_validate()\n",
    "\n",
    "\n",
    "# # 10-fold cross-validation accuracy, precision, recall and f1 average scores \n",
    "scorers['SVC(rbf kernel) with dirty data']=CS595ModelClassificationScoring(copy.deepcopy(svc), X_dirty, y_dirty, 'SVC(rbf kernel) with dirty data')\n",
    "scorers['SVC(rbf kernel) with dirty data'].cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance  of SVM with non-linear kernal\n",
    "The cross-validation performance of the SVM model with non-linear kernal(rbf) is the best among all the models tried till now. This indicates that there is some form of non linearity in the dataset which could not be captured accurately using polynomial features but is better captured by Gaussian/RBF Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with features selected by RandomizedLogisticRegression\n",
    "\n",
    "Logistic regression is a very powerful technique but its very sensitive to outliers and features with poor predictive power. So first I select  good features using  a technique know as stability selection. randomized selection technique where the penalty of a random subset of coefficients has been scaled. By performing this double randomization several times, the method assigns high scores to features that are repeatedly selected across randomizations. \n",
    "\n",
    "\"Randomized Logistic Regression works by subsampling the training data and fitting a L1-penalized LogisticRegression model where the penalty of a random subset of coefficients has been scaled. By performing this double randomization several times, the method assigns high scores to features that are repeatedly selected across randomizations.\" - sklearn documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of features selected from clean data: 15\n",
      "racepctblack, racePctWhite, racePctHisp, pctUrban, pctWInvInc, FemalePctDiv, TotalPctDiv, PctFam2Par, PctKids2Par, PctPersDenseHous, PctHousLess3BR, HousVacant, PctHousOccup, MedRentPctHousInc, LemasPctOfficDrugUn\n",
      "\n",
      "Number of features selected from dirty data: 14\n",
      "racePctWhite, racePctHisp, pctUrban, pctWInvInc, MalePctDivorce, FemalePctDiv, TotalPctDiv, PctFam2Par, PctKids2Par, PctPersDenseHous, HousVacant, PctHousOccup, MedRentPctHousInc, LemasPctOfficDrugUn\n",
      "\n",
      "Logistic Regression with clean data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the Logistic Regression with clean data model are: \n",
      "\taccuracy              : 0.828900(±0.05)\n",
      "\tprecision             : 0.869400(±0.05)\n",
      "\trecall                : 0.863200(±0.1)\n",
      "\tf1                    : 0.860700(±0.05)\n",
      "\n",
      "Logistic Regression with clean data performace on training with full data:\n",
      "Accuracy of the model is 0.8394380331159057\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.78      0.79      0.79       743\n",
      "       True       0.87      0.87      0.87      1250\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1993\n",
      "\n",
      "\n",
      "The 10 most predictive features of the Logistic Regression with clean data are : \n",
      "                   Feature Importance\n",
      "racepctblack                 2.685487\n",
      "TotalPctDiv                  2.161631\n",
      "racePctWhite                 2.135897\n",
      "PctKids2Par                  1.766958\n",
      "pctWInvInc                   1.624321\n",
      "HousVacant                   1.611573\n",
      "racePctHisp                  1.531708\n",
      "PctPersDenseHous             1.428298\n",
      "FemalePctDiv                 1.424136\n",
      "MedRentPctHousInc            1.267230\n",
      "\n",
      "Logistic Regression with dirty data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the Logistic Regression with dirty data model are: \n",
      "\taccuracy              : 0.841000(±0.02)\n",
      "\tprecision             : 0.875300(±0.02)\n",
      "\trecall                : 0.871300(±0.02)\n",
      "\tf1                    : 0.873100(±0.01)\n",
      "\n",
      "Logistic Regression with dirty data performace on training with full data:\n",
      "Accuracy of the model is 0.8415245737211635\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.79      0.79       743\n",
      "          1       0.87      0.87      0.87      1251\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1994\n",
      "\n",
      "\n",
      "The 10 most predictive features of the Logistic Regression with dirty data are : \n",
      "                   Feature Importance\n",
      "racePctWhite                 3.495924\n",
      "PctKids2Par                  1.990629\n",
      "MalePctDivorce               1.729816\n",
      "HousVacant                   1.692842\n",
      "pctWInvInc                   1.674814\n",
      "MedRentPctHousInc            1.492277\n",
      "TotalPctDiv                  1.415919\n",
      "PctPersDenseHous             1.363808\n",
      "FemalePctDiv                 1.093794\n",
      "racePctHisp                  1.069404\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RandomizedLogisticRegression, LogisticRegression\n",
    "\n",
    "# setting variables for easy access\n",
    "X_clean, y_clean = clean_data_df[predictor_variables], clean_data_df[target_variable_clf]\n",
    "X_dirty, y_dirty = full_data_df[predictor_variables], full_data_df[target_variable_clf]\n",
    "\n",
    "# initializing the classification scoring instance for decision tree\n",
    "randomized_logistic_clean = RandomizedLogisticRegression(random_state=210).fit(X_clean, y_clean)\n",
    "randomized_logistic_dirty = RandomizedLogisticRegression(random_state=210).fit(X_dirty, y_dirty)\n",
    "\n",
    "selected_clean_features = [k for k, v in dict(zip(predictor_variables,randomized_logistic_clean.get_support())).items() if v]\n",
    "selected_dirty_features = [k for k, v in dict(zip(predictor_variables,randomized_logistic_dirty.get_support())).items() if v]\n",
    "\n",
    "print(\"\\nNumber of features selected from clean data: {}\".format(len(selected_clean_features)))\n",
    "print(', '.join(selected_clean_features))\n",
    "print(\"\\nNumber of features selected from dirty data: {}\".format(len(selected_dirty_features)))\n",
    "print(', '.join(selected_dirty_features))\n",
    "\n",
    "X_clean = clean_data_df[selected_clean_features]\n",
    "X_dirty = full_data_df[selected_dirty_features]\n",
    "\n",
    "# initializing the classification scoring instance for SVM with \n",
    "logistic_regression = LogisticRegression()\n",
    "log_reg_model_name_1 =  'Logistic Regression with clean data'\n",
    "# 10-fold cross-validation accuracy, precision, recall and f1 average scores \n",
    "scorers[log_reg_model_name_1]=CS595ModelClassificationScoring(copy.deepcopy(logistic_regression), X_clean, y_clean, \n",
    "                                                                               log_reg_model_name_1)\n",
    "scorers[log_reg_model_name_1].cross_validate()\n",
    "\n",
    "scorers[log_reg_model_name_1].full_data_fit_score(print_report='clf')\n",
    "show_feature_importances(np.abs(scorers[log_reg_model_name_1].full_model.coef_.T.flatten()), selected_clean_features, log_reg_model_name_1)\n",
    "\n",
    "log_reg_model_name_2 =  'Logistic Regression with dirty data'\n",
    "# 10-fold cross-validation accuracy, precision, recall and f1 average scores \n",
    "scorers[log_reg_model_name_2]=CS595ModelClassificationScoring(copy.deepcopy(logistic_regression), X_dirty, y_dirty, \n",
    "                                                                               log_reg_model_name_2)\n",
    "scorers[log_reg_model_name_2].cross_validate()\n",
    "\n",
    "scorers[log_reg_model_name_2].full_data_fit_score(print_report='clf')\n",
    "show_feature_importances(np.abs(scorers[log_reg_model_name_2].full_model.coef_.T.flatten()), selected_dirty_features, log_reg_model_name_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Logistic regression\n",
    "Here we can see that performace is almost outperforms the best performing model SVM (rbf kernel) at a fraction of the model complexity and number of features used. This shows how powerful logistic regression can be and how important feature selection is in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier - Randomizing the random forest\n",
    "Extremely randomized trees are a modification of the random forest classifer where not only is a random subset of candidate features used, but  the thresholds also are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. According to sklearn documentation this usually allows to reduce the variance of the model more than random forest models, at the expense of an increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ExtraTrees with clean data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the ExtraTrees with clean data model are: \n",
      "\taccuracy              : 0.784200(±0.05)\n",
      "\tprecision             : 0.831600(±0.05)\n",
      "\trecall                : 0.829600(±0.1)\n",
      "\tf1                    : 0.825700(±0.05)\n",
      "\n",
      "ExtraTrees with clean data performace on training with full data:\n",
      "Accuracy of the model is 1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00       743\n",
      "       True       1.00      1.00      1.00      1250\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1993\n",
      "\n",
      "\n",
      "The 10 most predictive features of the ExtraTrees with clean data are : \n",
      "                Feature Importance\n",
      "racepctblack              0.048917\n",
      "PctIlleg                  0.039900\n",
      "pctWInvInc                0.039097\n",
      "FemalePctDiv              0.034273\n",
      "TotalPctDiv               0.029157\n",
      "PctTeen2Par               0.028904\n",
      "pctWPubAsst               0.026477\n",
      "MalePctDivorce            0.026340\n",
      "racePctHisp               0.026108\n",
      "PctKids2Par               0.022023\n",
      "\n",
      "ExtraTrees with dirty data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the ExtraTrees with dirty data model are: \n",
      "\taccuracy              : 0.817500(±0.01)\n",
      "\tprecision             : 0.863700(±0.03)\n",
      "\trecall                : 0.843300(±0.02)\n",
      "\tf1                    : 0.852900(±0.01)\n",
      "\n",
      "ExtraTrees with dirty data performace on training with full data:\n",
      "Accuracy of the model is 1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       743\n",
      "          1       1.00      1.00      1.00      1251\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1994\n",
      "\n",
      "\n",
      "The 10 most predictive features of the ExtraTrees with dirty data are : \n",
      "                Feature Importance\n",
      "racepctblack              0.048180\n",
      "PctIlleg                  0.040518\n",
      "pctWInvInc                0.038857\n",
      "FemalePctDiv              0.033666\n",
      "TotalPctDiv               0.029561\n",
      "PctTeen2Par               0.028996\n",
      "pctWPubAsst               0.026530\n",
      "MalePctDivorce            0.026279\n",
      "racePctHisp               0.025502\n",
      "racePctWhite              0.021976\n"
     ]
    }
   ],
   "source": [
    "# setting variables for easy access\n",
    "X_clean, y_clean = clean_data_df[predictor_variables], clean_data_df[target_variable_clf]\n",
    "X_dirty, y_dirty = full_data_df[predictor_variables], full_data_df[target_variable_clf]\n",
    "\n",
    "# initializing the classification scoring instance for extra trees\n",
    "extra_trees =  ExtraTreesClassifier(n_estimators=20, random_state=210)\n",
    "# 10-fold cross-validation accuracy, precision, recall and f1 average scores \n",
    "scorers['ExtraTrees with clean data']=CS595ModelClassificationScoring(copy.deepcopy(extra_trees), X_clean, y_clean, 'ExtraTrees with clean data')\n",
    "scorers['ExtraTrees with clean data'].cross_validate()\n",
    "\n",
    "scorers['ExtraTrees with clean data'].full_data_fit_score(print_report='clf')\n",
    "show_feature_importances(np.abs(scorers['ExtraTrees with clean data'].full_model.feature_importances_), predictor_variables, 'ExtraTrees with clean data')\n",
    "\n",
    "# # 10-fold cross-validation accuracy, precision, recall and f1 average scores \n",
    "scorers['ExtraTrees with dirty data']=CS595ModelClassificationScoring(copy.deepcopy(extra_trees), X_dirty, y_dirty, 'ExtraTrees with dirty data')\n",
    "scorers['ExtraTrees with dirty data'].cross_validate()\n",
    "\n",
    "scorers['ExtraTrees with dirty data'].full_data_fit_score(print_report='clf')\n",
    "show_feature_importances(np.abs(scorers['ExtraTrees with dirty data'].full_model.feature_importances_), predictor_variables, 'ExtraTrees with dirty data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Extra Trees\n",
    "Here we can see that performace of the extra trees is good and the feature importance matches features selected from other models and feature selection techniques. But the performace is still below logistic regression and SVM with rbf kernel.  I am sure this model can perform better with the right tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier \n",
    "Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient Tree Boosting models are used in a variety of areas including Web search ranking and ecology. - sklean documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Boosting with clean data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the Gradient Boosting with clean data model are: \n",
      "\taccuracy              : 0.819800(±0.06)\n",
      "\tprecision             : 0.859200(±0.06)\n",
      "\trecall                : 0.861600(±0.1)\n",
      "\tf1                    : 0.854900(±0.05)\n",
      "\n",
      "Gradient Boosting with clean data performace on training with full data:\n",
      "Accuracy of the model is 0.870546914199699\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.82      0.84      0.83       743\n",
      "       True       0.90      0.89      0.90      1250\n",
      "\n",
      "avg / total       0.87      0.87      0.87      1993\n",
      "\n",
      "\n",
      "The 10 most predictive features of the Gradient Boosting with clean data are : \n",
      "                  Feature Importance\n",
      "PctKids2Par                 0.338200\n",
      "racePctWhite                0.122595\n",
      "NumIlleg                    0.105232\n",
      "FemalePctDiv                0.071176\n",
      "racePctHisp                 0.063432\n",
      "PctIlleg                    0.051780\n",
      "PctPersDenseHous            0.038144\n",
      "HousVacant                  0.031833\n",
      "TotalPctDiv                 0.013028\n",
      "blackPerCap                 0.010296\n",
      "\n",
      "Gradient Boosting with dirty data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the Gradient Boosting with dirty data model are: \n",
      "\taccuracy              : 0.835000(±0.02)\n",
      "\tprecision             : 0.867600(±0.02)\n",
      "\trecall                : 0.870500(±0.01)\n",
      "\tf1                    : 0.868900(±0.01)\n",
      "\n",
      "Gradient Boosting with dirty data performace on training with full data:\n",
      "Accuracy of the model is 0.8706118355065195\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.84      0.83       743\n",
      "          1       0.90      0.89      0.90      1251\n",
      "\n",
      "avg / total       0.87      0.87      0.87      1994\n",
      "\n",
      "\n",
      "The 10 most predictive features of the Gradient Boosting with dirty data are : \n",
      "                  Feature Importance\n",
      "PctKids2Par                 0.338256\n",
      "racePctWhite                0.122603\n",
      "NumIlleg                    0.105257\n",
      "FemalePctDiv                0.071182\n",
      "racePctHisp                 0.063395\n",
      "PctIlleg                    0.051802\n",
      "PctPersDenseHous            0.038140\n",
      "HousVacant                  0.031821\n",
      "TotalPctDiv                 0.013023\n",
      "blackPerCap                 0.010294\n"
     ]
    }
   ],
   "source": [
    "# setting variables for easy access\n",
    "X_clean, y_clean = clean_data_df[predictor_variables], clean_data_df[target_variable_clf]\n",
    "X_dirty, y_dirty = full_data_df[predictor_variables], full_data_df[target_variable_clf]\n",
    "\n",
    "grad_boost_model_name = 'Gradient Boosting with clean data'\n",
    "# initializing the classification scoring instance for Gradient Boosting Classifier \n",
    "gradient_boosting =  GradientBoostingClassifier(n_estimators=20 , random_state=210)\n",
    "# 10-fold cross-validation accuracy, precision, recall and f1 average scores \n",
    "scorers[grad_boost_model_name]=CS595ModelClassificationScoring(copy.deepcopy(gradient_boosting), X_clean, y_clean, grad_boost_model_name)\n",
    "scorers[grad_boost_model_name].cross_validate()\n",
    "\n",
    "scorers[grad_boost_model_name].full_data_fit_score(print_report='clf')\n",
    "show_feature_importances(np.abs(scorers[grad_boost_model_name].full_model.feature_importances_), predictor_variables, grad_boost_model_name)\n",
    "\n",
    "# # 10-fold cross-validation accuracy, precision, recall and f1 average scores \n",
    "scorers['Gradient Boosting with dirty data']=CS595ModelClassificationScoring(copy.deepcopy(gradient_boosting), X_dirty, y_dirty, 'Gradient Boosting with dirty data')\n",
    "scorers['Gradient Boosting with dirty data'].cross_validate()\n",
    "\n",
    "scorers['Gradient Boosting with dirty data'].full_data_fit_score(print_report='clf')\n",
    "show_feature_importances(np.abs(scorers['Gradient Boosting with dirty data'].full_model.feature_importances_), predictor_variables, 'Gradient Boosting with dirty data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Gradient Boosting Classifier\n",
    "Here we can see that performace of gradient boosting is slightly better than that of random forest and but is still below the logistic regression with feature selection and SVM with RBF kernel. I am sure this model can perform better with the right tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feature selection using recursive feature elimination\n",
    "Features are ranked with recursive feature elimination and cross-validated selection of the best number of features. These features can be used in place of all the features and gain almost all the predictive power possible using the classifier. I have used a SVC with linear kernel as the classifier. I wanted to check what the effect of feature selection would be on SVM models which tended to be the best preformes yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection of optimal number of features for clean data\n",
      "Number of optimal features selected: 7\n",
      "\tSelected Features\n",
      "\t1. racepctblack\n",
      "\t2. racePctWhite\n",
      "\t3. racePctHisp\n",
      "\t4. numbUrban\n",
      "\t5. MalePctDivorce\n",
      "\t6. PctKids2Par\n",
      "\t7. HousVacant\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEbCAYAAADJWrOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n24SruwK7TAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2468a91c550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC linear kernel with CV feature selection on clean data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the SVC linear kernel with CV feature selection on clean data model are: \n",
      "\taccuracy              : 0.836900(±0.05)\n",
      "\tprecision             : 0.881000(±0.05)\n",
      "\trecall                : 0.863200(±0.1)\n",
      "\tf1                    : 0.866400(±0.05)\n",
      "Selection of optimal number of features for dirty data\n",
      "Number of optimal features selected: 25\n",
      "\tSelected Features\n",
      "\t1. population\n",
      "\t2. householdsize\n",
      "\t3. racepctblack\n",
      "\t4. racePctWhite\n",
      "\t5. racePctHisp\n",
      "\t6. agePct12t21\n",
      "\t7. agePct65up\n",
      "\t8. numbUrban\n",
      "\t9. pctWWage\n",
      "\t10. pctWInvInc\n",
      "\t11. NumUnderPov\n",
      "\t12. PctEmplProfServ\n",
      "\t13. PctOccupManu\n",
      "\t14. MalePctDivorce\n",
      "\t15. TotalPctDiv\n",
      "\t16. PctFam2Par\n",
      "\t17. PctKids2Par\n",
      "\t18. PctYoungKids2Par\n",
      "\t19. PctIlleg\n",
      "\t20. PersPerOccupHous\n",
      "\t21. HousVacant\n",
      "\t22. PctHousNoPhone\n",
      "\t23. RentLowQ\n",
      "\t24. RentHighQ\n",
      "\t25. LemasSwornFT\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEbCAYAAADJWrOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhjHGGE+swDDGGOOJFRjGGGM8+X+IeEgtivWVMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2468ab305f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC linear kernel with CV feature selection on dirty data performace on cross validation:\n",
      "\n",
      "The 10-fold cross-validation accuracy, precision, recall, f1 for the SVC linear kernel with CV feature selection on dirty data model are: \n",
      "\taccuracy              : 0.843500(±0.01)\n",
      "\tprecision             : 0.881800(±0.02)\n",
      "\trecall                : 0.867300(±0.02)\n",
      "\tf1                    : 0.874300(±0.01)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "def fit_RFECV(model_object, train_data, target_data):\n",
    "    # Create the RFE object and compute a cross-validated score.\n",
    "    # The \"accuracy\" scoring is used to select the features\n",
    "    rfecv = RFECV(estimator=model_object, step=1, cv=StratifiedKFold(),scoring='accuracy')\n",
    "    rfecv.fit(train_data, target_data)\n",
    "    feature_ranks = sorted(list(zip(predictor_variables_dirty, rfecv.ranking_ )), key=lambda x :x[1], reverse=False)\n",
    "    selected_features = []\n",
    "    print(\"Number of optimal features selected: {}\".format(rfecv.n_features_))\n",
    "    print ('\\tSelected Features')\n",
    "    for i, feature_rank in enumerate(feature_ranks[:rfecv.n_features_]):\n",
    "        print (\"\\t{}. {}\".format(i+1, feature_rank[0]))\n",
    "        selected_features.append(feature_rank[0])\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# setting variables for easy access\n",
    "X_clean, y_clean = clean_data_df[predictor_variables], clean_data_df[target_variable_clf]\n",
    "X_dirty, y_dirty = full_data_df[predictor_variables], full_data_df[target_variable_clf]\n",
    "\n",
    "# Classifier used for RFECV\n",
    "svc_lin = SVC(kernel=\"linear\", random_state =210, cache_size=400)\n",
    "\n",
    "print(\"Selection of optimal number of features for clean data\")\n",
    "clean_features_selected = fit_RFECV(copy.deepcopy(svc_lin), X_clean, y_clean)\n",
    "\n",
    "model_name_svc_4 = 'SVC linear kernel with CV feature selection on clean data'\n",
    "# 10-fold cross-validation accuracy, precision, recall and f1 average scores \n",
    "scorers[model_name_svc_4]=CS595ModelClassificationScoring(copy.deepcopy(svc_lin), \n",
    "                                                                          clean_data_df[clean_features_selected], y_clean, \n",
    "                                                                          model_name_svc_4)\n",
    "scorers[model_name_svc_4].cross_validate()\n",
    "\n",
    "\n",
    "model_name_svc_5 = 'SVC linear kernel with CV feature selection on dirty data'\n",
    "print(\"Selection of optimal number of features for dirty data\")\n",
    "dirty_features_selected = fit_RFECV(copy.deepcopy(svc_lin), X_dirty, y_dirty)\n",
    "               \n",
    "\n",
    "# # 10-fold cross-validation accuracy, precision, recall and f1 average scores \n",
    "scorers[model_name_svc_5] = CS595ModelClassificationScoring(copy.deepcopy(svc_lin), \n",
    "                                                                      full_data_df[dirty_features_selected], y_dirty, \n",
    "                                                                      model_name_svc_5)\n",
    "scorers[model_name_svc_5].cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of SVM with linear kernel and cross-validated feature selection\n",
    "Here we can see that performace of SVM esily outperforms the best performing models using only a fraction of the predictor variables for both data sets. This again highlights how important feature selection is in the process of model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Best peforming model\n",
    "For me the best performing model is Logistic Regression with cross-validation based feature selection. This model is not only very fast to compute but it is also easier to interpret. It is a bit unfair to compare it to other models that have not been tuned such as decision trees, extra trees and gradient boosting. But I prefer this because it gives a sufficiently high level of performance without needing many predictor variables and usinf minimal computation resources. So even though the SVC linear kernel with CV feature selection model slighthly outperforms the logistic regression I chose it because the model is a simpler one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common high importance features\n",
    "The top 5 most features that have consistent high importance are:\n",
    "1.PctKids2Par\n",
    "2. racePctWhite\n",
    "3. racePctHisp \n",
    "4. HousVacant\n",
    "5. MalePctDivorce\n",
    "\n",
    "Most of the models have differnt approches in how they treat correlated variables and how they calculate importance. All these methods capture different forms of predictive power that the varibles possess. So if these variables are showing up consistently as high importance features in multiple models and also when feature selection techniques are applied then its highly reliable that these variables are actually of high importance in predicting crime. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
